{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pseudo data to stimulate different fMRI parcellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausumming there are 500000 voxels in a fMRI scan\n",
    "# We create 3 different parcellations:\n",
    "# 1. 50 parcels, each parcel has 10000 voxels\n",
    "# 2. 500 parcels, each parcel has 1000 voxels\n",
    "# 3. 500 parcels, each parcel has different number of voxels, from 1 to 1000, but the total number of voxels is still 500000\n",
    "\n",
    "# Pseudo data is created by sci-kit learn make_classification function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# hyperparameters\n",
    "N_SAMPLES = 800\n",
    "N_INFORMATIVE_RATIO = 0.01\n",
    "\n",
    "# Create 50 parcels, each parcel has 10000 voxels\n",
    "parcel_50 = []\n",
    "for i in range(50):\n",
    "    parcel_50.append(make_classification(n_samples=N_SAMPLES, \n",
    "                                         n_features=10000, \n",
    "                                         n_informative=int(10000*N_INFORMATIVE_RATIO),\n",
    "                                         n_classes=8,\n",
    "    ))\n",
    "parcel_50 = np.array(parcel_50)\n",
    "\n",
    "# Create 500 parcels, each parcel has 1000 voxels\n",
    "parcel_500 = []\n",
    "for i in range(500):\n",
    "    parcel_500.append(make_classification(n_samples=N_SAMPLES,\n",
    "                                          n_features=1000,\n",
    "                                          n_informative=int(1000*N_INFORMATIVE_RATIO),\n",
    "                                          n_classes=8,\n",
    "    ))\n",
    "                                          \n",
    "parcel_500 = np.array(parcel_500)\n",
    "\n",
    "# Create 500 parcels, each parcel has different number of voxels(features), from 1 to 1000, but the total number of voxels(features) is still 500000\n",
    "parcel_500_diff = []\n",
    "\n",
    "# get random number of features for each parcel, sum for all parcels is 500000\n",
    "random_features_num = np.random.randint(1, 1000, 500)\n",
    "random_features_num = random_features_num / sum(random_features_num) * 500000\n",
    "random_features_num = random_features_num.astype(int)\n",
    "print('Total number of voxels is: ', sum(random_features_num))\n",
    "\n",
    "for i in range(500):\n",
    "    parcel_500_diff.append(make_classification(n_samples=N_SAMPLES,\n",
    "                                               n_features=random_features_num[i],\n",
    "                                               n_informative=int(random_features_num[i]*N_INFORMATIVE_RATIO),\n",
    "                                               n_classes=8,\n",
    "    ))\n",
    "\n",
    "                                            \n",
    "parcel_500_diff = np.array(parcel_500_diff)\n",
    "\n",
    "# Save the data\n",
    "with open('parcel_50.pickle', 'wb') as f:\n",
    "    pickle.dump(parcel_50, f)\n",
    "with open('parcel_500.pickle', 'wb') as f:\n",
    "    pickle.dump(parcel_500, f)\n",
    "with open('parcel_500_diff.pickle', 'wb') as f:\n",
    "    pickle.dump(parcel_500_diff, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the performance of sklearn and cuML (No parallelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance of sklearn and cuML by evaluating the accuracy of the model and \n",
    "# the time it takes to train the model of different parcellations\n",
    "\n",
    "# # Load the data\n",
    "# with open('parcel_50.pickle', 'rb') as f:\n",
    "#     parcel_50 = pickle.load(f)\n",
    "# with open('parcel_500.pickle', 'rb') as f:\n",
    "#     parcel_500 = pickle.load(f)\n",
    "# with open('parcel_500_diff.pickle', 'rb') as f:\n",
    "#     parcel_500_diff = pickle.load(f)\n",
    "\n",
    "# Create a list of different parcellations\n",
    "parcellations = [parcel_50, parcel_500, parcel_500_diff]\n",
    "\n",
    "# Performance log\n",
    "df = pd.DataFrame(columns=['parcel', 'n_features', 'n_informative', 'n_classes', 'n_samples', 'sklearn_time', 'sklearn_accuracy', 'cuml_time', 'cuml_accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate with sklearn and cuML with cross validation\n",
    "from sklearn.linear_model import RandomForestClassifier as skRandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from cuml.ensemble import RandomForestClassifier as cuRandomForestClassifier\n",
    "\n",
    "for i in range(len(parcellations)):\n",
    "    for j in range(len(parcellations[i])):\n",
    "        # sklearn\n",
    "        start = time.time()\n",
    "        sk_model = skRandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "        scores = cross_validate(sk_model, parcellations[i][j][0], parcellations[i][j][1], cv=5, scoring='accuracy')\n",
    "        end = time.time()\n",
    "        sklearn_time = end - start\n",
    "        sklearn_accuracy = scores['test_score'].mean()\n",
    "        \n",
    "        # cuML\n",
    "        start = time.time()\n",
    "        cu_model = cuRandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "        scores = cross_validate(cu_model, parcellations[i][j][0], parcellations[i][j][1], cv=5, scoring='accuracy')\n",
    "        end = time.time()\n",
    "        cuml_time = end - start\n",
    "        cuml_accuracy = scores['test_score'].mean()\n",
    "        \n",
    "        # log\n",
    "        df = df.append({'parcel': i, 'n_features': parcellations[i][j][0].shape[1], 'n_informative': parcellations[i][j][2], 'n_classes': parcellations[i][j][3], 'n_samples': parcellations[i][j][0].shape[0], 'sklearn_time': sklearn_time, 'sklearn_accuracy': sklearn_accuracy, 'cuml_time': cuml_time, 'cuml_accuracy': cuml_accuracy}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
