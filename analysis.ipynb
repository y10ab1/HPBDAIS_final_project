{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pseudo data to stimulate different fMRI parcellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming there are 500000 voxels in a fMRI scan\n",
    "\n",
    "\n",
    "# Pseudo data is created by sci-kit learn make_classification function\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from cuml import make_classification\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "# Get k parcels(patches) with same number of voxels(features)\n",
    "def get_parcels(X, y, k, dtype='cp'):\n",
    "    \"\"\"\n",
    "    X: numpy or cupy array, shape = (n_samples, n_features)\n",
    "    y: numpy or cupy array, shape = (n_samples, )\n",
    "    k: int, number of patches\n",
    "    \"\"\"\n",
    "    # get number of voxels in each parcel\n",
    "    n_voxels_per_parcel = int(X.shape[1]/k)\n",
    "    # get k parcels\n",
    "    parcels = []\n",
    "    if dtype == 'cp':\n",
    "        for i in range(k):\n",
    "            parcels.append((X[:, i*n_voxels_per_parcel:(i+1)*n_voxels_per_parcel], y))\n",
    "    else:\n",
    "        for i in range(k):\n",
    "            parcels.append((X[:, i*n_voxels_per_parcel:(i+1)*n_voxels_per_parcel].get(), y.get()))\n",
    "        \n",
    "    return parcels\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Get k parcels(patches) with random number of voxels(features) in each parcel\n",
    "def get_parcels_diff(X, y, k=300, least_voxels_per_parcel=100, dtype='cp'):\n",
    "    \"\"\"\n",
    "    X: numpy or cupy array, shape = (n_samples, n_features)\n",
    "    y: numpy or cupy array, shape = (n_samples, )\n",
    "    k: int, number of parcels\n",
    "    least_voxels_per_parcel: int, the least number of voxels in each parcel\n",
    "    \"\"\"\n",
    "    # get k parcels\n",
    "    parcels = []\n",
    "    if dtype == 'cp':\n",
    "        for i in range(k):\n",
    "            # get number of voxels in each parcel\n",
    "            n_voxels_per_parcel = cp.random.randint(least_voxels_per_parcel, X.shape[1]//k)\n",
    "            parcels.append((X[:, i*n_voxels_per_parcel:(i+1)*n_voxels_per_parcel], y))\n",
    "    else:\n",
    "        for i in range(k):\n",
    "            # get number of voxels in each parcel\n",
    "            n_voxels_per_parcel = np.random.randint(least_voxels_per_parcel, X.shape[1]//k)\n",
    "            parcels.append((X[:, i*n_voxels_per_parcel:(i+1)*n_voxels_per_parcel].get(), y.get()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the performance of sklearn and cuML (No parallelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance of sklearn and cuML by evaluating the accuracy of the model and \n",
    "# the time it takes to train the model of different parcellations\n",
    "\n",
    "# hyperparameters\n",
    "N_SAMPLES = 500 # 500 or 3000\n",
    "N_INFORMATIVE_RATIO = 0.01 # 1% of voxels are informative\n",
    "N_CLASSES = 8\n",
    "DATA_TYPE = np.float32 # set data type to float32 to leaverage GPU\n",
    "N_FEATURES = 300000\n",
    "N_PATCHES = 300 # 20 or 300\n",
    "SKLEARN_N_JOBS = 1 # number of jobs for sklearn\n",
    "PARCEL_VOXELS = 'same' # 'same' or 'diff'\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=N_SAMPLES, \n",
    "                            n_features=N_FEATURES, \n",
    "                            n_informative=int(N_FEATURES*N_INFORMATIVE_RATIO),\n",
    "                            n_classes=N_CLASSES)\n",
    "\n",
    "\n",
    "# Create a list of different parcellations\n",
    "parcellations = [get_parcels(X, y, N_PATCHES)]\n",
    "parcellations_name = {}\n",
    "\n",
    "# Performance log\n",
    "df = pd.DataFrame(columns=['parcel_type', 'n_features', 'n_informative', 'n_classes', 'n_samples', 'sklearn_time', 'sklearn_accuracy', 'cuml_time', 'cuml_accuracy'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:29<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 27s, sys: 1.45 s, total: 3min 28s\n",
      "Wall time: 3min 29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train and evaluate with sklearn with cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier as skRandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from cuml.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in range(len(parcellations)):\n",
    "    for j in tqdm(range(len(parcellations[i]))):\n",
    "        # sklearn\n",
    "        start = time.time()\n",
    "        sk_model = skRandomForestClassifier(n_estimators=100, max_depth=2, random_state=0, n_jobs=SKLEARN_N_JOBS)\n",
    "        scores = cross_validate(sk_model, parcellations[i][j][0].get(), parcellations[i][j][1].get(), cv=5, scoring='accuracy')\n",
    "        # scores = cross_validate(sk_model, parcellations[i][j][0], parcellations[i][j][1], cv=5, scoring='accuracy')\n",
    "        end = time.time()\n",
    "        sklearn_time = end - start\n",
    "        sklearn_accuracy = scores['test_score'].mean()\n",
    "\n",
    "        \n",
    "        # log\n",
    "        df = pd.concat([df, pd.DataFrame.from_records([{'parcel_type': 'sklearn_non_parallel',\n",
    "                                            'n_features': parcellations[i][j][0].shape[1],\n",
    "                                            'n_informative': N_INFORMATIVE_RATIO*parcellations[i][j][0].shape[1],\n",
    "                                            'n_classes': N_CLASSES,\n",
    "                                            'n_samples': N_SAMPLES,\n",
    "                                            'sklearn_time': sklearn_time,\n",
    "                                            'sklearn_accuracy': sklearn_accuracy,\n",
    "                                            }])], ignore_index=True)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the log\n",
    "# df.to_csv('performance_log_no_parallel.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the performance of sklearn and cuML (Parallelization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Train and evaluate with cuML with cross validation\n",
    "# from cuml.ensemble import RandomForestClassifier as cuRandomForestClassifier\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# kfold = KFold(n_splits=5)\n",
    "\n",
    "# for i in range(len(parcellations)):\n",
    "#     for j in tqdm(range(len(parcellations[i]))):\n",
    "\n",
    "#         # cuML\n",
    "#         fold_accuracy = cp.array([])\n",
    "#         # X, y = cp.array(parcellations[i][j][0]), cp.array(parcellations[i][j][1])\n",
    "#         start = time.time()\n",
    "#         for train_idx, test_idx in kfold.split(X=parcellations[i][j][0], y=parcellations[i][j][1]):\n",
    "#             cu_model = cuRandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "#             cu_model.fit(parcellations[i][j][0][train_idx], parcellations[i][j][1][train_idx])\n",
    "#             # cu_model.fit(X[train_idx], y[train_idx])\n",
    "#             fold_accuracy = cp.append(fold_accuracy, accuracy_score(parcellations[i][j][1][test_idx], cu_model.predict(parcellations[i][j][0][test_idx])))\n",
    "#         end = time.time()\n",
    "#         cuml_time = end - start\n",
    "#         cuml_accuracy = cp.asnumpy(fold_accuracy).mean()\n",
    "        \n",
    "#         # log\n",
    "#         df = pd.concat([df, pd.DataFrame.from_records([{'parcel_type': 'cuML',\n",
    "#                             'n_features': parcellations[i][j][0].shape[1],\n",
    "#                             'n_informative': N_INFORMATIVE_RATIO*parcellations[i][j][0].shape[1],\n",
    "#                             'n_classes': N_CLASSES,\n",
    "#                             'n_samples': N_SAMPLES,\n",
    "#                             'cuml_time': cuml_time,\n",
    "#                             'cuml_accuracy': cuml_accuracy,\n",
    "#                             }])], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn (Parallelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 729 ms, sys: 1 s, total: 1.73 s\n",
      "Wall time: 43.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcel_type</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_informative</th>\n",
       "      <th>n_classes</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>sklearn_time</th>\n",
       "      <th>sklearn_accuracy</th>\n",
       "      <th>cuml_time</th>\n",
       "      <th>cuml_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sklearn_non_parallel</td>\n",
       "      <td>1000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>0.760647</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sklearn_non_parallel</td>\n",
       "      <td>1000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>0.707530</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sklearn_non_parallel</td>\n",
       "      <td>1000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>0.693067</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sklearn_non_parallel</td>\n",
       "      <td>1000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>0.696927</td>\n",
       "      <td>0.1520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sklearn_non_parallel</td>\n",
       "      <td>1000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>0.702187</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>sklearn_parallel</td>\n",
       "      <td>1000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>0.734852</td>\n",
       "      <td>0.1524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>sklearn_parallel</td>\n",
       "      <td>1000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>0.726679</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>sklearn_parallel</td>\n",
       "      <td>1000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>0.741448</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>sklearn_parallel</td>\n",
       "      <td>1000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>0.725690</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>sklearn_parallel</td>\n",
       "      <td>1000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>0.776262</td>\n",
       "      <td>0.1548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              parcel_type n_features  n_informative n_classes n_samples  \\\n",
       "0    sklearn_non_parallel       1000           10.0         8       500   \n",
       "1    sklearn_non_parallel       1000           10.0         8       500   \n",
       "2    sklearn_non_parallel       1000           10.0         8       500   \n",
       "3    sklearn_non_parallel       1000           10.0         8       500   \n",
       "4    sklearn_non_parallel       1000           10.0         8       500   \n",
       "..                    ...        ...            ...       ...       ...   \n",
       "355      sklearn_parallel       1000           10.0         8       500   \n",
       "356      sklearn_parallel       1000           10.0         8       500   \n",
       "357      sklearn_parallel       1000           10.0         8       500   \n",
       "358      sklearn_parallel       1000           10.0         8       500   \n",
       "359      sklearn_parallel       1000           10.0         8       500   \n",
       "\n",
       "     sklearn_time  sklearn_accuracy cuml_time cuml_accuracy  \n",
       "0        0.760647            0.1440       NaN           NaN  \n",
       "1        0.707530            0.1420       NaN           NaN  \n",
       "2        0.693067            0.1580       NaN           NaN  \n",
       "3        0.696927            0.1520       NaN           NaN  \n",
       "4        0.702187            0.1480       NaN           NaN  \n",
       "..            ...               ...       ...           ...  \n",
       "355      0.734852            0.1524       NaN           NaN  \n",
       "356      0.726679            0.1528       NaN           NaN  \n",
       "357      0.741448            0.1480       NaN           NaN  \n",
       "358      0.725690            0.1408       NaN           NaN  \n",
       "359      0.776262            0.1548       NaN           NaN  \n",
       "\n",
       "[360 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# parallelize the training and evaluation process for sklearn\n",
    "\n",
    "BATCH_SIZE = 5 # number of models to train at a time\n",
    "N_JOBS = 4 # number of CPUs to use for a RandomForestClassifier\n",
    "\n",
    "\n",
    "# Define a function to train a model and return its score\n",
    "def train_and_log(i, j, parcellation):\n",
    "    # sklearn\n",
    "    start = time.time()\n",
    "    sk_model = skRandomForestClassifier(n_estimators=100, max_depth=2, random_state=0, n_jobs=N_JOBS)\n",
    "    scores = cross_validate(sk_model, parcellation[0].get(), parcellation[1].get(), cv=5, scoring='accuracy')\n",
    "    # scores = cross_validate(sk_model, parcellation[0], parcellation[1], cv=5, scoring='accuracy')\n",
    "    end = time.time()\n",
    "    sklearn_time = end - start\n",
    "    sklearn_accuracy = scores['test_score'].mean()\n",
    "\n",
    "    # log\n",
    "    return {'parcel_type': 'sklearn_parallel',\n",
    "            'n_features': parcellation[0].shape[1],\n",
    "            'n_informative': N_INFORMATIVE_RATIO*parcellation[0].shape[1],\n",
    "            'n_classes': N_CLASSES,\n",
    "            'n_samples': N_SAMPLES,\n",
    "            'sklearn_time': sklearn_time,\n",
    "            'sklearn_accuracy': sklearn_accuracy,\n",
    "            }\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Define the number of models to train at a time as the number of CPUs\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "# Flatten the list of parcellations and associated indices\n",
    "flat_parcellations = [(i, j, parcellations[i][j]) for i in range(len(parcellations)) for j in range(len(parcellations[i]))]\n",
    "\n",
    "# Use joblib to train models and log results in batches\n",
    "for i in range(0, len(flat_parcellations), batch_size):\n",
    "    batch_parcellations = flat_parcellations[i:i+batch_size]\n",
    "    start = time.time()\n",
    "    batch_results = Parallel(n_jobs=-1)(delayed(train_and_log)(*parcellation) for parcellation in batch_parcellations)\n",
    "    end = time.time()\n",
    "    \n",
    "    average_results = {'parcel_type': 'sklearn_parallel',\n",
    "            'n_features': parcellations[0][0][0].shape[1],\n",
    "            'n_informative': N_INFORMATIVE_RATIO*parcellations[0][0][0].shape[1],\n",
    "            'n_classes': N_CLASSES,\n",
    "            'n_samples': N_SAMPLES,\n",
    "            'sklearn_time': (end - start),\n",
    "            'sklearn_accuracy': np.mean([result['sklearn_accuracy'] for result in batch_results])}\n",
    "            \n",
    "    \n",
    "    results.extend([average_results])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df = pd.concat([df, pd.DataFrame.from_records(results)], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the log\n",
    "# df.to_csv(f'result_nsapmles{N_SAMPLES}_npaches{N_PATCHES}_parcelsVoxels_{PARCEL_VOXELS}.csv', index=False)\n",
    "df.to_csv(f'n_job1_result_nsapmles{N_SAMPLES}_npaches{N_PATCHES}_parcelsVoxels_{PARCEL_VOXELS}.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cuML (Parallelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # parallelize the training and evaluation process for cuML\n",
    "\n",
    "# BATCH_SIZE = 5 # number of models to train at a time\n",
    "# N_JOBS = 1 # number of CPUs to use for a RandomForestClassifier\n",
    "\n",
    "\n",
    "# # Define a function to train a model and return its score\n",
    "# def train_and_log(i, j, parcellation):\n",
    "#     # cuML\n",
    "#     fold_accuracy = cp.array([])\n",
    "#     # start = time.time()\n",
    "#     # for train_idx, test_idx in kfold.split(X=parcellation[0], y=parcellation[1]):\n",
    "#     #     cu_model = cuRandomForestClassifier(n_estimators=100, max_depth=2)\n",
    "#     #     cu_model.fit(parcellation[0][train_idx], parcellation[1][train_idx])\n",
    "#     #     fold_accuracy = cp.append(fold_accuracy, accuracy_score(parcellation[1][test_idx], cu_model.predict(parcellation[0][test_idx])))\n",
    "#     # end = time.time()\n",
    "#     start = time.time()\n",
    "#     cu_model = cuRandomForestClassifier(n_estimators=100, max_depth=2)\n",
    "#     scores = cross_validate(cu_model, parcellation[0].get(), parcellation[1].get(), cv=5, scoring='accuracy')\n",
    "#     end = time.time()\n",
    "#     cuml_time = end - start\n",
    "#     cuml_accuracy = cp.asnumpy(fold_accuracy).mean()\n",
    "\n",
    "#     # log\n",
    "#     return {'parcel_type': i,\n",
    "#             'n_features': parcellation[0].shape[1],\n",
    "#             'n_informative': N_INFORMATIVE_RATIO*parcellation[0].shape[1],\n",
    "#             'n_classes': N_CLASSES,\n",
    "#             'n_samples': N_SAMPLES,\n",
    "#             'cuml_time': cuml_time,\n",
    "#             'cuml_accuracy': cuml_accuracy,\n",
    "#             }\n",
    "    \n",
    "    \n",
    "# # Create a list to store the results\n",
    "# results = []\n",
    "\n",
    "# # Define the number of models to train at a time as the number of CPUs\n",
    "# batch_size = BATCH_SIZE\n",
    "\n",
    "# # Flatten the list of parcellations and associated indices\n",
    "# flat_parcellations = [(i, j, parcellations[i][j]) for i in range(len(parcellations)) for j in range(len(parcellations[i]))]\n",
    "\n",
    "# # Use joblib to train models and log results in batches\n",
    "# for i in range(0, len(flat_parcellations), batch_size):\n",
    "#     batch_parcellations = flat_parcellations[i:i+batch_size]\n",
    "#     batch_results = Parallel(n_jobs=-1)(delayed(train_and_log)(*parcellation) for parcellation in batch_parcellations)\n",
    "#     results.extend(batch_results)\n",
    "\n",
    "# # Convert results to DataFrame\n",
    "# df = pd.DataFrame(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
